---
title: "Understanding ODELO"
date: "2025-04-30"
categories: [Footy,ODELO, Introduction,Explainer]
description: "An introduction to the model I use to make predictions"
execute:
    echo: False
    code-tools: True
    warning: False
image: "Test.png"
jupyter: systempy
---

![](Test.png)



Welcome to Holy Grail Ratings. This is a project of mine to try and model AFL margins as best as possible. There's a lot of work left to do on this project which I'll discuss at the end of the post. But for now I think its best to explain how the model currently works, why I've made particular choices, and how to interpret the dashboards on the homepage.

# The Importance of Scoring Shots

To introduce my model for AFL margins consider one question - what is the principal component of a football game’s margin? If you asked head coaches you’d probably get a variety of answers reflecting gameplan - maybe pressure rating, uncontested marks, handball/kick ratio. My model takes a simpler view which relies on a heuristic observation about AFL, an average game has an extremely high number of interactions. A key result? Events at the extremes tend to cancel each other, for every goal conceded from an intercepted kick-in there’s a set shot from a questionable free kick. The implication for AFL is that scoring is relatively stable, extreme events tend to have a minimal effect on the final margin. What matters most then? – scoring shots, any attempt on goal, whether a major, behind or clanger [^1].  Here's a chart stating the obvious teams who get more shots than their opponent win far more,
```{python}
echo: True
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression
import pandas as pd
import matplotlib.dates as mdates
```


```{python}
echo: False
error: False

data = pd.read_csv('teamdata.csv')

data['ss_diff'] = data['T1SS'] - data['T2SS'] 
data['won'] = data['T1S'] > data['T2S']
data = data.loc[data['ss_diff'] >= 0]

ratios = {}

for i in range(0,11):
    #calculate the win rate
    filtered = data.loc[data['ss_diff'] == i]
    won = filtered.loc[filtered['won'] == True]
    ratio = round(len(won)/len(filtered),1)
    ratios[i] = ratio

df = pd.DataFrame(list(ratios.items()), columns=['ss_diff', 'percentage'])

sns.set(style="whitegrid")  # Optional for nicer styling


plt.figure(figsize=(10, 6))  # Optional: control figure size
sns.barplot(data=df, x='ss_diff', y='percentage', palette='crest')

plt.title("Win Ratio and scoring shot differentials")
plt.xlabel("Scoring Shot Differential")
plt.ylabel("Win Percentage")
plt.xticks()  # Rotate x-labels if needed
plt.tight_layout()
plt.show()
```

```{python}
echo: False
# Example: your dataframe

df = pd.read_csv('teamdata.csv')
df['ss_diff'] = df['T1SS'] - df['T2SS'] 
#data['won'] = df['T1S'] > df['T2S']
#df = df.loc[df['ss_diff'] >= 0]

df = df[1::2]

# df should have columns: 'ScoringShotDiff' and 'Margin'
# Replace with your actual dataframe name
X = df['ss_diff'].values.reshape(-1, 1)
y = df['Margin'].values

# Fit linear regression model
model = LinearRegression()
model.fit(X, y)

slope = model.coef_[0]
intercept = model.intercept_
line_eq = f"y = {slope:.2f}x + {intercept:.2f}"

# Plot scatter and regression line
plt.figure(figsize=(10, 6))
sns.scatterplot(x='ss_diff', y='Margin', data=df)
sns.regplot(x='ss_diff', y='Margin', data=df, scatter=False, label=line_eq, color='red')

# Add formula as text on the plot
plt.text(0.05, 0.95, line_eq, transform=plt.gca().transAxes,
         fontsize=12, verticalalignment='top', color='red')

plt.title("Scoring Shot Differential vs Margin")
plt.xlabel("Scoring Shot Differential")
plt.ylabel("Margin")
plt.legend()
plt.tight_layout()
plt.show()
```

A logical question is, what about accuracy? Are the best teams more accurate and the worst more inaccurate, scoring shot differentials potentially just represent a good teams dominance on the field. Surprisingly, teams good or bad rarely manage to outperform or underperform compared to the league average scoring shot conversion. Check out this graph of each teams conversion compared to the league average.

```{python}
df = pd.read_csv('teamdata.csv')
# Calculate conversion rate
df['conversion_rate'] = df['T1G'] / df['T1SS']

# Sort by team and game number
df = df.sort_values(['Team1', 'MasterGame'])

# Calculate 10-game moving average
df['moving_avg'] = df.groupby('Team1')['conversion_rate'].transform(lambda x: x.rolling(window=10, min_periods=1).mean())

# Plot each team
plt.figure(figsize=(14, 8))

for team in df['Team1'].unique():
    team_data = df[df['Team1'] == team]
    team_data = team_data.iloc[10:]  # skip first 10 rows for each team
    plt.plot(team_data['MasterGame'], team_data['moving_avg'], label=team)

plt.title("10-Game Moving Average of Conversion Rate by Team")
plt.xlabel("Game Number")
plt.ylabel("Conversion Rate (Score / Scoring Shots)")
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.tight_layout()
plt.grid(True)
plt.show()

```


It is true however that certain shots are better than others, shots from the goal square vs centre square will have wildly different conversion rates. Thus comes XScore, in my model I use an excellent calculation by AFLLab which weights the value of a shot in comparison to the historical conversion for that shot based on shot position.[^2] With some stats about scoring shots under our belt it’s time to think about turning it into margin predictions.

# ODELO Explainer
ODELO (Offensive and Defensive Elo) is the model I use to generate team ratings. A team has two ratings, offensive and defensive, measured as the number of points better than the league average number of scoring shots converted at the average rate. Ratings in this form give us an idea of the relative strengths of teams and allows a simple comparison across teams. They also can be easily turned into predictions of the margin by comparing the number of points better an offense is than its opposing defence; to convert to margins the amount the home team offense will outscore the away defense is calculated, the defensive margin is calculated similarly and the final expected margin is just the sum.

$$
\text{Expected Home Offensive Margin} = \text{Home Team Offensive Rating} - \text{Away Team Defensive Rating}
$$
$$
\text{Expected Home Defensive Margin} = \text{Home Team Defensive Rating} - \text{Away Team Offensive Rating}
$$
$$
\text{Expected Home Margin} = \frac{\text{Expected Home Offensive Margin} + \text{Expected Home Defensive Margin}}{2}
$$

ODELO is an Elo system where we use these predicted margins to assess a team’s actual performance and adjust their ratings. Elo systems were developed for chess, and are a well-known technique for assessing the relative strength of teams and players in different sports.  The trick to Elo systems is updating ratings, in chess two players ratings are updated after the game using this formula,

$$
\text{New Rating} = \text{Old Rating} + K(\text{Expected Performance} - \text{Actual Performance})
$$

In chess, Elo ratings are arbitrarily set such that an average player has a rating of 1500 and scaled so that a 200 point rating difference translates to a 75% win probability. In ODELO the ratings are scaled to reflect points relative to the average score, and I have fit my own win probability function. To update ODELO ratings, I have a slightly modified formula, with several parameters. The formulas simply adjusts a teams rating by the amount they outperform the average score,

$$
\text{New Offensive Rating} = \text{Old Offensive Rating} + K(\text{Home Expected Score} - \text{Average Score})
$$
$$
\text{New Defensive Rating} = \text{Old Defensive Rating} + K(\text{Average Score} - \text{Away Expected Score})
$$

There are three parameters in the calculation required  for optimisation, 
1.	K-factor
2.	Regression to mean
3.	Days to Average

### K-factor

K-factor weights a team’s actual performance compared to their expected performance and determines the size of rating changes. I use an exponentially decaying K-factor dependent on the proportion of the season passed. This reflects some obvious observations about AFL, early in the season there’s not much known about a team’s ability, there’s likely personnel change, game plan change, aging. Similarly, towards the end of the season there is more certainty around a team’s ability, the prevalence of dead rubber games in the last few rounds also gives us reason to not move ratings more. Currently I have lazily optimised this by finding a co-efficient which minimises MAE but this will need improvement and currently performs worse than a constant K-factor. It looks like this, 

```{python}
# Parameters
A = 1       # initial value
k = 3.75     # decay rate
t = np.linspace(0, 1, 200)  # time range from 0 to 28

# Decaying exponential function
y = A * np.exp(-k * t)

# Plot
plt.figure(figsize=(8, 5))
plt.plot(t, y, color='teal')
plt.title("K-factor thorugh the season")
plt.xlabel("Proportion of season passed")
plt.ylabel("K-factor")
plt.xlim(0, 1)  # restrict x-axis range
plt.grid(False)
plt.legend()
plt.tight_layout()
plt.show()
```


### Regression to mean

To account for inter-season effects, it is helpful to shift teams back towards the mean, this accounts well for several small effects which are hard to quantify on their own but clearly have an effect on the game, for example teams which finish a season on a hot run tend to come back to reality as other teams study their gameplan. It also accounts for equalisation measures imposed across the league. Teams offensive and defensive ratings are regressed 30% in between seasons, which is a simple brute force optimisation, and decreases MAE significantly,

### Days to average

This is a simple measure to account for long term changes in the average score, the average score of AFL is relatively stable, with no major short term variations with the exception of the covid season.

```{python}
df = pd.read_csv('teamdata.csv')
# Ensure dates are datetime
df['date'] = pd.to_datetime(df['date'])

# Compute rolling average
df['rolling_avg_score'] = df['T1S'].rolling(window=150).mean()

# Plot using MasterGames for spacing, but with dates for labeling
plt.figure(figsize=(12, 6))
plt.plot(df['MasterGame'], df['rolling_avg_score'], color='navy', label='30-Game Rolling Avg')

# Set date labels using MasterGames positions
# Choose ticks every ~6 months
tick_spacing = 180  # adjust this depending on data density
tick_locs = df['MasterGame'][::tick_spacing]  # slice every N games
tick_labels = df['date'].dt.strftime('%b %Y')[::tick_spacing]  # format date labels

plt.xticks(tick_locs, tick_labels, rotation=45)

# Add labels and formatting
plt.title("Rolling Average Score vs. MasterGames")
plt.xlabel("Date")
plt.ylabel("Average Score")
plt.grid(False)
plt.tight_layout()
plt.legend()
plt.show()

```

So it is reasonable to use it in the model but the window to determine the average score is optimised to account for any long term systemic trends. This comes out to about x years.

# Win Probability

I mentioned earlier that chess uses a set formula to convert Elo ratings into implied win probabilities, so does ODELO. Chess Elo’s conversion to win probabilities relies on a key assumption that player performance in a particular game is normally distributes (though now often it is assumed to be log-normal). This forms the basis of Chess’ win probability calculation. I have chosen a different approach fitting a logistic regression of predicted margins to final margins, this returns a sigmoid coefficient.[^3] This returns a sigmoid function and coefficient which yields this nice graph showing the implied win probability given an expected margin.

```{python}
# Steepness parameter
k = 0.026433  # You can adjust this to see different shapes

# Generate x values
x = np.linspace(-120, 120, 400)

# Generalized sigmoid function
sigmoid = 1 / (1 + np.exp(-k * x))

# Plot
fig, ax = plt.subplots(figsize=(8, 5))
ax.plot(x, sigmoid, color='crimson')

# Axes limits
ax.set_xlim(-120, 120)
ax.set_ylim(0, 1)

# Customize axes
ax.spines['left'].set_position('zero')
ax.spines['bottom'].set_position(('data', 0.5))
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

# Labels and title
ax.set_xlabel("x", loc='right')
ax.set_ylabel("σ(x)", loc='top')
ax.set_title(f"Sigmoid Function (k = {k})")
ax.legend()

plt.grid(False)
plt.tight_layout()
plt.show()

```

# Performance

Back testing with these parameters ODELO has a mean absolute error of 28.8 over 2014 – 2024. This is fairly solid given there are a number of simple improvements to be made but comparing it with other models from Squiggle where the best models tend to have an error of 22 - 27 depending on the year there is plenty of error to trim. 

# Future Improvements

ODELO remains fairly rough, and I have a few improvements to work on. Most obvious is an estimate of home ground advantage, I've done some early research on this and plan to write a future post on it. Improving optimisation is another goal, K-factor being the main offender, given its importance to weightings I suspect any improvement will significantly improve performance. Asides from these two I have some other ideas to investigate,

1.	A bye adjustment and more broadly a measure to account for differing amounts of rest between teams
2.	A player based model to blend with ODELO to account for injuries and individual contributions
3.	Optimising win probability based on expected total score[^4]
4.	Various adjustments about team performance in finals
5.	Determine a weighting of XScore and true score, some amount of true score probably reflects a team’s ability to get good shots or perform under pressure.

These are a few of the big things that I’m planning to add into the model which will probably take some time but are nonetheless achievable. Hopefully this gave some insight into how the predictions displayed on the home page are calculated and what exactly they represent.

### Data

In case you’re interested in how I manage the data for the model and the sources used. I have an SQL database which I populate with data sourced in python using the fitzRoypy wrapper for the fitzRoy API. I draw data from a few sources for forecasting but for optimising to historical data I use game data from Squiggle. I’ve also experimented with historical betting data as a way to measure error, this dataset (ausbetting table) is the best source I’ve been able to find, if you have a better one please get in touch!

[^1]: I've taken this scoring shots terminology from Matter of Stats.
[^2]: [This is the explainer for the Xscore calculations I use.](https://theafllab.wordpress.com/2021/03/24/introducing-aflxscore/)
[^3]: This is because I want the model to account for all adjustments rather than the win probability function.
[^4]: [This article explains the observed correlation between win probability and expected margin and totals.](https://www.matterofstats.com/mafl-stats-journal/2024/7/8/the-relationship-between-expected-victory-margins-and-estimated-win-probabilities)

